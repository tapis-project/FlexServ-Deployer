{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "systems = {\n",
    "    \"vista\": {\n",
    "        \"node_defaults\": {\n",
    "            # Grace/Grace (GG) nodes (CPU-only)\n",
    "            \"gg\": {\n",
    "                \"node_kind\": \"cpu\",\n",
    "                \"cpu_model\": \"NVIDIA Grace CPU Superchip\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 72,\n",
    "                \"cores_per_node\": 144,\n",
    "                \"memory_gb\": 237,  # LPDDR\n",
    "            },\n",
    "            # Grace/Hopper (GH) nodes (GPU)\n",
    "            \"gh\": {\n",
    "                \"node_kind\": \"gpu\",\n",
    "                \"cpu_model\": \"NVIDIA Grace CPU\",\n",
    "                \"cpus_per_node\": 1,\n",
    "                \"cores_per_cpu\": 72,\n",
    "                \"cores_per_node\": 72,\n",
    "                \"cpu_memory_gb\": 116,  # LPDDR5X/LPDDR per doc\n",
    "                \"gpu_model\": \"NVIDIA H200\",\n",
    "                \"gpus_per_node\": 1,\n",
    "                \"gpu_memory_gb\": 96,  # HBM3\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"cpu_partitions\": {\n",
    "            \"dev\": {},\n",
    "            \"prod\": {\n",
    "                \"gg\": {\n",
    "                    \"node_type\": \"gg\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 32,\n",
    "                    \"assoc_cores_per_node\": 144,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 128,\n",
    "                    \"max_jobs_per_user\": 20,\n",
    "                    \"max_submit\": 40,\n",
    "                    \"charge_rate_su_per_node_hour\": 0.33,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"gpu_partitions\": {\n",
    "            \"dev\": {\n",
    "                \"gh-dev\": {\n",
    "                    \"node_type\": \"gh\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 8,\n",
    "                    \"assoc_cores_per_node\": 72,\n",
    "                    \"assoc_gpus_per_node\": 1,\n",
    "                    \"max_job_duration_hours\": 2,\n",
    "                    \"max_nodes_per_user\": 8,\n",
    "                    \"max_jobs_per_user\": 1,\n",
    "                    \"max_submit\": 3,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"gh\": {\n",
    "                    \"node_type\": \"gh\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 64,\n",
    "                    \"assoc_cores_per_node\": 72,\n",
    "                    \"assoc_gpus_per_node\": 1,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 192,\n",
    "                    \"max_jobs_per_user\": 20,\n",
    "                    \"max_submit\": 40,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"stampede3\": {\n",
    "        \"node_defaults\": {\n",
    "            \"skx\": {\n",
    "                \"node_kind\": \"cpu\",\n",
    "                \"cpu_model\": \"Intel Xeon Platinum 8160 (Skylake)\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 24,\n",
    "                \"cores_per_node\": 48,\n",
    "                \"memory_gb\": 192,\n",
    "            },\n",
    "            \"icx\": {\n",
    "                \"node_kind\": \"cpu\",\n",
    "                \"cpu_model\": \"Intel Xeon Platinum 8380 (Ice Lake)\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 40,\n",
    "                \"cores_per_node\": 80,\n",
    "                \"memory_gb\": 256,\n",
    "            },\n",
    "            \"spr\": {\n",
    "                \"node_kind\": \"cpu\",\n",
    "                \"cpu_model\": \"Intel Xeon CPU MAX 9480 (Sapphire Rapids HBM)\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 56,\n",
    "                \"cores_per_node\": 112,\n",
    "                \"memory_gb\": 128,  # HBM2e\n",
    "            },\n",
    "            \"nvdimm\": {\n",
    "                \"node_kind\": \"cpu\",\n",
    "                \"cpu_model\": \"Intel Xeon Platinum 8380 (Ice Lake) + NVDIMM\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 40,\n",
    "                \"cores_per_node\": 80,\n",
    "                \"memory_gb\": 4096,  # 4TB NVDIMM (per doc section)\n",
    "            },\n",
    "            \"h100\": {\n",
    "                \"node_kind\": \"gpu\",\n",
    "                \"cpu_model\": \"Intel Xeon Platinum 8468 (Sapphire Rapids)\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 48,\n",
    "                \"cores_per_node\": 96,\n",
    "                \"memory_gb\": 1024,  # 1TB DDR5\n",
    "                \"gpu_model\": \"NVIDIA H100 SXM5\",\n",
    "                \"gpus_per_node\": 4,\n",
    "                \"gpu_memory_gb\": 96,\n",
    "            },\n",
    "            \"pvc\": {\n",
    "                \"node_kind\": \"gpu\",\n",
    "                \"cpu_model\": \"Intel Xeon Platinum 8480 (Sapphire Rapids)\",\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cores_per_cpu\": 48,\n",
    "                \"cores_per_node\": 96,\n",
    "                \"memory_gb\": 1024,  # 1TB DDR5\n",
    "                \"gpu_model\": \"Intel Data Center GPU Max 1550 (PVC)\",\n",
    "                \"gpus_per_node\": 4,\n",
    "                \"gpu_memory_gb\": 124,  # 62GB per tile; 2 tiles => 124GB per GPU\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"cpu_partitions\": {\n",
    "            \"dev\": {\n",
    "                \"skx-dev\": {\n",
    "                    \"node_type\": \"skx\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 16,\n",
    "                    \"assoc_cores_per_node\": 48,\n",
    "                    \"max_job_duration_hours\": 2,\n",
    "                    \"max_nodes_per_user\": 16,\n",
    "                    \"max_jobs_per_user\": 2,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"skx\": {\n",
    "                    \"node_type\": \"skx\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 256,\n",
    "                    \"assoc_cores_per_node\": 48,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 256,\n",
    "                    \"max_jobs_per_user\": 40,\n",
    "                    \"max_submit\": 60,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "                \"icx\": {\n",
    "                    \"node_type\": \"icx\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 32,\n",
    "                    \"assoc_cores_per_node\": 80,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 48,\n",
    "                    \"max_jobs_per_user\": 12,\n",
    "                    \"max_submit\": 20,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.5,\n",
    "                },\n",
    "                \"spr\": {\n",
    "                    \"node_type\": \"spr\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 32,\n",
    "                    # assoc cores for spr isn't fully visible in the snippet,\n",
    "                    # but the system spec is 112 cores/node; keep it consistent.\n",
    "                    \"assoc_cores_per_node\": 112,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    # per-user limits not fully visible in snippet; leave as None\n",
    "                    \"max_nodes_per_user\": None,\n",
    "                    \"max_jobs_per_user\": None,\n",
    "                    \"max_submit\": None,\n",
    "                    \"charge_rate_su_per_node_hour\": None,\n",
    "                },\n",
    "                \"nvdimm\": {\n",
    "                    \"node_type\": \"nvdimm\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 1,\n",
    "                    \"assoc_cores_per_node\": 80,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 1,\n",
    "                    \"max_jobs_per_user\": 2,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 4.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"gpu_partitions\": {\n",
    "            \"dev\": {\n",
    "                # Stampede3 doc table snippet doesnâ€™t show an explicit dev GPU queue.\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"h100\": {\n",
    "                    \"node_type\": \"h100\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 4,\n",
    "                    \"assoc_cores_per_node\": 96,\n",
    "                    \"assoc_gpus_per_node\": 4,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 4,\n",
    "                    \"max_jobs_per_user\": 2,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 4.0,\n",
    "                },\n",
    "                \"pvc\": {\n",
    "                    \"node_type\": \"pvc\",\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 4,\n",
    "                    \"assoc_cores_per_node\": 96,\n",
    "                    \"assoc_gpus_per_node\": 4,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 4,\n",
    "                    \"max_jobs_per_user\": 2,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 3.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"ls6\": {\n",
    "        \"node_defaults\": {\n",
    "            \"cpu\": {\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cpu_model\": \"AMD EPYC 7763 (Milan)\",\n",
    "                \"cores_per_cpu\": 64,\n",
    "                \"cores_per_node\": 128,\n",
    "                \"memory_gb\": 256,\n",
    "            },\n",
    "            \"gpu\": {\n",
    "                \"cpus_per_node\": 2,\n",
    "                \"cpu_model\": \"AMD EPYC 7763 (Milan)\",\n",
    "                \"cores_per_cpu\": 64,\n",
    "                \"cores_per_node\": 128,\n",
    "                \"memory_gb\": 256,\n",
    "            },\n",
    "        },\n",
    "        \"cpu_partitions\": {\n",
    "            \"dev\": {\n",
    "                \"development\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 8,\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 2,\n",
    "                    \"max_nodes_per_user\": 8,\n",
    "                    \"max_jobs_per_user\": 1,\n",
    "                    \"max_submit\": 3,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"normal\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 64,\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 75,\n",
    "                    \"max_jobs_per_user\": 20,\n",
    "                    \"max_submit\": 100,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "                \"large\": {\n",
    "                    \"min_nodes\": 65,\n",
    "                    \"max_nodes\": 256,\n",
    "                    \"cores_per_node\": 256,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 256,\n",
    "                    \"max_jobs_per_user\": 1,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.0,\n",
    "                },\n",
    "                \"vm-small\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 1,\n",
    "                    \"cores_per_node\": 16,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 4,\n",
    "                    \"max_jobs_per_user\": 4,\n",
    "                    \"max_submit\": 16,\n",
    "                    \"charge_rate_su_per_node_hour\": 0.143,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"gpu_partitions\": {\n",
    "            \"dev\": {\n",
    "                \"gpu-a100-dev\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 2,\n",
    "                    \"gpus_per_node\": 3,  # LS6 A100 nodes: 3 GPUs/node :contentReference[oaicite:2]{index=2}\n",
    "                    \"gpu_model\": \"NVIDIA A100\",\n",
    "                    \"gpu_memory_gb\": 40,\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 2,\n",
    "                    \"max_nodes_per_user\": 2,\n",
    "                    \"max_jobs_per_user\": 1,\n",
    "                    \"max_submit\": 3,\n",
    "                    \"charge_rate_su_per_node_hour\": 3.0,\n",
    "                },\n",
    "            },\n",
    "            \"prod\": {\n",
    "                \"gpu-a100\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 8,\n",
    "                    \"gpus_per_node\": 3,  \n",
    "                    \"gpu_model\": \"NVIDIA A100\",\n",
    "                    \"gpu_memory_gb\": 40,\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 12,\n",
    "                    \"max_jobs_per_user\": 8,\n",
    "                    \"max_submit\": 32,\n",
    "                    \"charge_rate_su_per_node_hour\": 3.0,\n",
    "                },\n",
    "                \"gpu-a100-small\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 1,\n",
    "                    \"gpus_per_node\": 3,  \n",
    "                    \"gpu_model\": \"NVIDIA A100\",\n",
    "                    \"gpu_memory_gb\": 40,\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 3,\n",
    "                    \"max_jobs_per_user\": 3,\n",
    "                    \"max_submit\": 12,\n",
    "                    \"charge_rate_su_per_node_hour\": 1.5,\n",
    "                },\n",
    "                \"gpu-h100\": {\n",
    "                    \"min_nodes\": 1,\n",
    "                    \"max_nodes\": 1,\n",
    "                    \"gpus_per_node\": 2,  \n",
    "                    \"gpu_model\": \"NVIDIA H100\",\n",
    "                    \"cores_per_node\": 128,\n",
    "                    \"max_job_duration_hours\": 48,\n",
    "                    \"max_nodes_per_user\": 1,\n",
    "                    \"max_jobs_per_user\": 1,\n",
    "                    \"max_submit\": 4,\n",
    "                    \"charge_rate_su_per_node_hour\": 6.0,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73381f8",
   "metadata": {},
   "outputs": [],
   "source": [
    ":dep tapis-authenticator = { git = \"https://github.com/tapis-project/tapis-rust-sdk\", package = \"tapis-authenticator\" }\n",
    ":dep tapis-pods = { git = \"https://github.com/tapis-project/tapis-rust-sdk\", package = \"tapis-pods\" }\n",
    ":dep serde_json = \"1.0.149\"\n",
    ":dep serde = { version = \"1.0.188\", features = [\"derive\"] }\n",
    ":dep sha2 = \"0.10.9\"\n",
    ":dep base62 = \"2.2.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05020d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "// :dep tapis-pods = { git = \"https://github.com/tapis-project/tapis-rust-sdk\", package = \"tapis-pods\" }\n",
    "// :dep serde_json = \"1.0.149\"\n",
    "// :dep serde = { version = \"1.0.188\", features = [\"derive\"] }\n",
    "\n",
    "use std::collections::HashMap;\n",
    "use tapis_pods;\n",
    "use serde_json::Value;\n",
    "use serde::{Serialize, Deserialize};\n",
    "use sha2::{Sha256, Digest};\n",
    "use base64::{engine::general_purpose::URL_SAFE_NO_PAD, Engine as _};\n",
    "\n",
    "\n",
    "/// Supported ML inference backends\n",
    "#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]\n",
    "#[serde(rename_all = \"lowercase\")]\n",
    "pub enum Backend {\n",
    "    #[serde(rename = \"transformers\")]\n",
    "    Transformers {command: Vec<String>},\n",
    "    #[serde(rename = \"vllm\")]\n",
    "    VLlm {command: Vec<String>},\n",
    "    #[serde(rename = \"sglang\")]\n",
    "    SGLang {command: Vec<String>},\n",
    "    #[serde(rename = \"trtllm\")]\n",
    "    TrtLlm {command: Vec<String>},\n",
    "}\n",
    "\n",
    "impl Backend {\n",
    "    pub fn as_str(&self) -> &str {\n",
    "        match self {\n",
    "            Backend::Transformers {..} => \"transformers\",\n",
    "            Backend::VLlm {..} => \"vllm\",\n",
    "            Backend::SGLang {..} => \"sglang\",\n",
    "            Backend::TrtLlm {..} => \"trtllm\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// Create a Transformers parameter builder\n",
    "    pub fn transformers(&self) -> TransformersParametersBuilder {\n",
    "        match self {\n",
    "            Backend::Transformers {command} => {\n",
    "                TransformersParametersBuilder::new(command.clone())\n",
    "            }\n",
    "            _ => panic!(\"Backend is not Transformers\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// Create a vLLM parameter builder\n",
    "    pub fn vllm(&self, command: Vec<String>) -> VLlmParametersBuilder {\n",
    "        match self {\n",
    "            Backend::VLlm {command} => {\n",
    "                VLlmParametersBuilder::new(command.clone())\n",
    "            },\n",
    "            _ => panic!(\"Backend is not vLLM\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// Create an SGLang parameter builder\n",
    "    pub fn sglang(&self, command: Vec<String>) -> SGLangParametersBuilder {\n",
    "        match self{\n",
    "            Backend::SGLang {command} => {\n",
    "                SGLangParametersBuilder::new(command.clone())\n",
    "            },\n",
    "            _ => panic!(\"Backend is not SGLang\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// Create a TRT-LLM parameter builder\n",
    "    pub fn trtllm(&self, command: Vec<String>) -> TrtLlmParametersBuilder {\n",
    "        match self {\n",
    "            Backend::TrtLlm {command} => {\n",
    "                TrtLlmParametersBuilder::new(command.clone())\n",
    "            },\n",
    "            _ => panic!(\"Backend is not TRT-LLM\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Backend-specific parameters\n",
    "/// This is a flexible JSON object that varies by backend\n",
    "#[derive(Debug, Clone, Serialize, Deserialize)]\n",
    "pub struct BackendParameters {\n",
    "    pub command: Vec<String>,\n",
    "    pub params: HashMap<String, Value>,\n",
    "    pub env: HashMap<String, String>,\n",
    "}\n",
    "\n",
    "impl BackendParameters {\n",
    "    pub fn new(command: Vec<String>) -> Self {\n",
    "        Self {\n",
    "            command,\n",
    "            params: HashMap::new(),\n",
    "            env: HashMap::new(),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pub fn insert_param<T: Serialize>(&mut self, key: impl Into<String>, value: T) -> &mut Self {\n",
    "        self.params\n",
    "            .insert(key.into(), serde_json::to_value(value).unwrap());\n",
    "        self\n",
    "    }\n",
    "\n",
    "    pub fn insert_env(&mut self, key: impl Into<String>, value: impl Into<String>) -> &mut Self {\n",
    "        self.env.insert(key.into(), value.into());\n",
    "        self\n",
    "    }\n",
    "\n",
    "    pub fn get(&self, key: &str) -> Option<&Value> {\n",
    "        self.params.get(key)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "/// Builder for transformers parameters\n",
    "pub struct TransformersParametersBuilder {\n",
    "    params: BackendParameters,\n",
    "}\n",
    "\n",
    "impl TransformersParametersBuilder {\n",
    "    pub fn new(command: Vec<String>) -> Self {\n",
    "        Self {\n",
    "            params: BackendParameters::new(command),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    pub fn default_model(mut self, model: &str) -> Self {\n",
    "        self.params.insert_param(\"default-model\", model);\n",
    "        self\n",
    "    }\n",
    "    pub fn default_embedding_model(mut self, model: &str) -> Self {\n",
    "        self.params.insert_param(\"default-embedding-model\", model);\n",
    "        self\n",
    "    }\n",
    "    pub fn host(mut self, host: &str) -> Self {\n",
    "        self.params.insert_param(\"host\", host);\n",
    "        self\n",
    "    }\n",
    "    pub fn port(mut self, port: u16) -> Self {\n",
    "        self.params.insert_param(\"port\", port);\n",
    "        self\n",
    "    }\n",
    "    pub fn device(mut self, device: &str) -> Self {\n",
    "        self.params.insert_param(\"device\", device);\n",
    "        self\n",
    "    }\n",
    "    pub fn dtype(mut self, dtype: &str) -> Self {\n",
    "        self.params.insert_param(\"dtype\", dtype);\n",
    "        self\n",
    "    }\n",
    "    pub fn continuous_batching(mut self, enabled: bool) -> Self {\n",
    "        self.params.insert_param(\"continuous-batching\", enabled);\n",
    "        self\n",
    "    }\n",
    "    pub fn flexserv_token(mut self, token: &str) -> Self {\n",
    "        self.params.insert_param(\"flexserv-token\", token);\n",
    "        self\n",
    "    }\n",
    "    pub fn force_default_model(mut self, force: bool) -> Self {\n",
    "        self.params.insert_param(\"force-default-model\", force);\n",
    "        self\n",
    "    }\n",
    "    pub fn force_default_embedding_model(mut self, force: bool) -> Self {\n",
    "        self.params.insert_param(\"force-default-embedding-model\", force);\n",
    "        self\n",
    "    }\n",
    "    pub fn log_level(mut self, level: &str) -> Self {\n",
    "        self.params.insert_param(\"log-level\", level);\n",
    "        self\n",
    "    }\n",
    "    pub fn quantization(mut self, quant: &str) -> Self {\n",
    "        self.params.insert_param(\"quantization\", quant);\n",
    "        self\n",
    "    }\n",
    "    pub fn trust_remote_code(mut self, trust: bool) -> Self {\n",
    "        self.params.insert_param(\"trust-remote-code\", trust);\n",
    "        self\n",
    "    }\n",
    "    pub fn attn_implementation(mut self, implementation: &str) -> Self {\n",
    "        self.params.insert_param(\"attn-implementation\", implementation);\n",
    "        self\n",
    "    }\n",
    "    pub fn enable_cors(mut self, enable: bool) -> Self {\n",
    "        self.params.insert_param(\"enable-cors\", enable);\n",
    "        self\n",
    "    }\n",
    "    pub fn non_blocking(mut self, non_blocking: bool) -> Self {\n",
    "        self.params.insert_param(\"non-blocking\", non_blocking);\n",
    "        self\n",
    "    }\n",
    "    pub fn build(self) -> BackendParameters {\n",
    "        self.params\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Builder for vLLM parameters\n",
    "pub struct VLlmParametersBuilder {\n",
    "    params: BackendParameters,\n",
    "}\n",
    "\n",
    "impl VLlmParametersBuilder {\n",
    "    pub fn new() -> Self {\n",
    "        Self {\n",
    "            params: BackendParameters::new(),\n",
    "        }\n",
    "    }\n",
    "    pub fn tensor_parallel_size(mut self, size: u32) -> Self {\n",
    "        self.params.insert(\"tensor_parallel_size\", size);\n",
    "        self\n",
    "    }\n",
    "    pub fn pipeline_parallel_size(mut self, size: u32) -> Self {\n",
    "        self.params.insert(\"pipeline_parallel_size\", size);\n",
    "        self\n",
    "    }\n",
    "    pub fn max_model_len(mut self, len: u32) -> Self {\n",
    "        self.params.insert(\"max_model_len\", len);\n",
    "        self\n",
    "    }\n",
    "    pub fn gpu_memory_utilization(mut self, util: f32) -> Self {\n",
    "        self.params.insert(\"gpu_memory_utilization\", util);\n",
    "        self\n",
    "    }\n",
    "    pub fn build(self) -> BackendParameters {\n",
    "        self.params\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Builder for SGLang parameters\n",
    "pub struct SGLangParametersBuilder {\n",
    "    params: BackendParameters,\n",
    "}\n",
    "\n",
    "impl SGLangParametersBuilder {\n",
    "    pub fn new() -> Self {\n",
    "        Self {\n",
    "            params: BackendParameters::new(),\n",
    "        }\n",
    "    }\n",
    "    pub fn tp_size(mut self, size: u32) -> Self {\n",
    "        self.params.insert(\"tp_size\", size);\n",
    "        self\n",
    "    }\n",
    "    pub fn mem_fraction_static(mut self, fraction: f32) -> Self {\n",
    "        self.params.insert(\"mem_fraction_static\", fraction);\n",
    "        self\n",
    "    }\n",
    "    pub fn build(self) -> BackendParameters {\n",
    "        self.params\n",
    "    }\n",
    "}\n",
    "\n",
    "/// Builder for TRT-LLM parameters\n",
    "pub struct TrtLlmParametersBuilder {\n",
    "    params: BackendParameters,\n",
    "}\n",
    "\n",
    "impl TrtLlmParametersBuilder {\n",
    "    pub fn new() -> Self {\n",
    "        Self {\n",
    "            params: BackendParameters::new(),\n",
    "        }\n",
    "    }\n",
    "    pub fn max_batch_size(mut self, size: u32) -> Self {\n",
    "        self.params.insert(\"max_batch_size\", size);\n",
    "        self\n",
    "    }\n",
    "    pub fn max_input_len(mut self, len: u32) -> Self {\n",
    "        self.params.insert(\"max_input_len\", len);\n",
    "        self\n",
    "    }\n",
    "    pub fn build(self) -> BackendParameters {\n",
    "        self.params\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "pub struct FlexServ {\n",
    "    /// tenant url\n",
    "    pub tenant_url: String,\n",
    "\n",
    "    /// tapis username\n",
    "    pub tapis_user: String,\n",
    "\n",
    "    /// model to deploy (e.g., \"meta-llama/Llama-3-70b-hf\")\n",
    "    pub default_model: String,\n",
    "\n",
    "    /// default embedding model\n",
    "    pub default_embedding_model: Option<String>,\n",
    "\n",
    "    /// backend to use\n",
    "    pub backend: Backend,\n",
    "    \n",
    "}\n",
    "impl FlexServ {\n",
    "    pub fn new(tenant_url: String, tapis_user: String, default_model: String, default_embedding_model: Option<String>, backend: Backend) -> Self {\n",
    "        FlexServ { tenant_url, tapis_user, default_model, default_embedding_model, backend }\n",
    "    }\n",
    "\n",
    "    pub fn deployment_hash(&self) -> String {\n",
    "        // Create a unique hash for the deployment configuration\n",
    "        let config_string = format!(\"{}@{}-{}-{:?}\", self.tapis_user, self.tenant_url, self.default_model, self.backend);\n",
    "        let digest = Sha256::digest(config_string.as_bytes());\n",
    "        let hash = \n",
    "        format!(\"{:x}\", sha256::compute(config_string))\n",
    "    }\n",
    "}\n",
    "\n",
    "pub struct FlexServPodDeployment {\n",
    "    new_volume: tapis_pods::models::NewVolume,\n",
    "    new_pod: tapis_pods::models::NewPod,\n",
    "    volume_info: tapis_pods::models::VolumeResponseModel,\n",
    "    pod_info: tapis_pods::models::PodResponseModel,\n",
    "    server: FlexServ,\n",
    "}\n",
    "\n",
    "pub struct FlexServHPCDeployment {\n",
    "    server: FlexServ,\n",
    "}\n",
    "/// Deployment result enum\n",
    "#[derive(Debug)]\n",
    "pub enum DeploymentResult {\n",
    "    PodResult {pod_info: tapis_pods::models::PodResponseModel, volume_info: tapis_pods::models::VolumeResponseModel, tapis_user: String, tapis_tenant: String, model_sha: String},\n",
    "    HPCResult {job_info: String, tapis_user: String, tapis_tenant: String, model_sha: String},\n",
    "}\n",
    "\n",
    "/// Deployment related errors\n",
    "/// We can bind the message to this enum variant for more detailed error information\n",
    "/// 1. TapisAuthFailed(String) - Authentication to Tapis failed\n",
    "/// 2. TapisAPIUnreachable(String) - Tapis API is unreachable\n",
    "/// 3. TapisBadRequest(String) - Bad request to Tapis API\n",
    "/// 4. TapisTimeout(String) - Request to Tapis API timed out\n",
    "/// 5. TapisInternalServerError(String) - Tapis API internal server error\n",
    "/// 6. UnkownError(String) - Unknown error\n",
    "/// 7. ModelUploadingFailed(String) - Model uploading failed not because of any of the reasons from 1-6.\n",
    "/// 8. PodCreationFailed(String) - Pod creation failed not be cause of any of the reasons from 1-6.\n",
    "/// 9. JobCreationFailed(String) - Job creation failed not be cause of any of the reasons from 1-6.\n",
    "#[derive(Debug)]\n",
    "pub enum DeploymentError {\n",
    "    TapisAuthFailed(String),\n",
    "    TapisAPIUnreachable(String),\n",
    "    TapisBadRequest(String),\n",
    "    TapisTimeout(String),\n",
    "    TapisInternalServerError(String),\n",
    "    UnkownError(String),\n",
    "    ModelUploadingFailed(String),\n",
    "    PodCreationFailed(String),\n",
    "    JobCreationFailed(String),\n",
    "}\n",
    "\n",
    "pub trait FlexServDeployment{\n",
    "    fn create(&mut self) -> Result<DeploymentResult, DeploymentError>;\n",
    "    fn start(&self)-> Result<DeploymentResult, DeploymentError>;\n",
    "    fn stop(&self)-> Result<DeploymentResult, DeploymentError>;\n",
    "    fn terminate(&self)-> Result<DeploymentResult, DeploymentError>;\n",
    "    fn monitor(&self)-> Result<DeploymentResult, DeploymentError>;\n",
    "}\n",
    "\n",
    "impl FlexServDeployment for FlexServPodDeployment {\n",
    "    fn create(&mut self) -> Result<DeploymentResult, DeploymentError> {\n",
    "        // Create volume and pod\n",
    "        todo!()\n",
    "    }\n",
    "\n",
    "    fn start(&self) -> Result<DeploymentResult, DeploymentError> {\n",
    "        // Start pod\n",
    "        todo!()\n",
    "    }\n",
    "\n",
    "    fn stop(&self) -> Result<DeploymentResult, DeploymentError> {\n",
    "        // Stop pod\n",
    "        todo!()\n",
    "    }\n",
    "\n",
    "    fn terminate(&self) -> Result<DeploymentResult, DeploymentError> {\n",
    "        // Terminate pod and delete volume\n",
    "        todo!()\n",
    "    }\n",
    "\n",
    "    fn monitor(&self) -> Result<DeploymentResult, DeploymentError> {\n",
    "        // Monitor pod status\n",
    "        todo!()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ec0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
